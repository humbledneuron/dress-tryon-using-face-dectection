{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\assets\\shirt0.png\n",
      ".\\assets\\shirt1.png\n",
      ".\\assets\\shirt2.png\n",
      ".\\assets\\shirt3.png\n",
      ".\\assets\\shirt4.png\n",
      ".\\assets\\shirt5.png\n",
      ".\\assets\\shirt6.png\n",
      ".\\assets\\shirt7.png\n",
      ".\\assets\\shirt8.png\n",
      ".\\assets\\shirt9.png\n",
      ".\\assets\\shirt10.png\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1122,1109,1) (0,1109,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 143\u001b[0m\n\u001b[0;32m    140\u001b[0m     cam\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 125\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    124\u001b[0m     dress_index \u001b[38;5;241m=\u001b[39m current_dress_index \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(dress_images)\n\u001b[1;32m--> 125\u001b[0m     \u001b[43mprocess_and_save_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdress_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     current_dress_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_dress_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dress_images):\n",
      "Cell \u001b[1;32mIn[4], line 90\u001b[0m, in \u001b[0;36mprocess_and_save_output\u001b[1;34m(frame, dress_index, detections, x, y, w, h)\u001b[0m\n\u001b[0;32m     88\u001b[0m frame_copy \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     89\u001b[0m dress_img \u001b[38;5;241m=\u001b[39m dress_images[dress_index]\n\u001b[1;32m---> 90\u001b[0m \u001b[43mapply_dress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdress_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(output_path, frame_copy)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaved output image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 77\u001b[0m, in \u001b[0;36mapply_dress\u001b[1;34m(frame, dress_img, x, y, w, h)\u001b[0m\n\u001b[0;32m     72\u001b[0m inv_dress_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m dress_mask\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Apply the dress overlay to the frame within the region of interest\u001b[39;00m\n\u001b[0;32m     75\u001b[0m frame[dress_roi_y_start:dress_roi_y_end, dress_roi_x_start:dress_roi_x_end, :\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     76\u001b[0m         dress_mask[:, :, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m*\u001b[39m dress_overlay[:, :, :\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m---> 77\u001b[0m         \u001b[43minv_dress_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdress_roi_y_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mdress_roi_y_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdress_roi_x_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mdress_roi_x_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     78\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1122,1109,1) (0,1109,3) "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#check  point 3.3 #left over will not be shown in camera and also the total transparent\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "# Load pre-trained face detection model\n",
    "prototxt_path = r'.\\computer_vision-master\\CAFFE_DNN\\deploy.prototxt'\n",
    "caffemodel_path = r'.\\computer_vision-master\\CAFFE_DNN\\res10_300x300_ssd_iter_140000.caffemodel'\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)\n",
    "\n",
    "# Initialize webcam\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "# Set frame width, height, and frame rate\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "cam.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "# Global variables\n",
    "dress_images = []\n",
    "\n",
    "user_picture_path = r'.\\assets'\n",
    "total_dresses = 11\n",
    "# Load dress images\n",
    "def select_dress_images():\n",
    "    global dress_images\n",
    "    dress_images.clear()\n",
    "    for i in range(total_dresses):\n",
    "        image_path = os.path.join(user_picture_path, f'shirt{i}.png')\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "        print(image_path)\n",
    "        dress_images.append(img)\n",
    "\n",
    "def apply_dress(frame, dress_img, x, y, w, h):\n",
    "    dress_h, dress_w = dress_img.shape[:2]\n",
    "\n",
    "    # Calculate the scale factors for resizing the dress\n",
    "    scale_factor_x = w / dress_w\n",
    "    scale_factor_y = h / dress_h\n",
    "\n",
    "    # Take the maximum scale factor to enlarge the dress as much as possible while fitting within the detected face\n",
    "    scale_factor = max(scale_factor_x, scale_factor_y)\n",
    "\n",
    "    # Resize the dress image to fit the detected face\n",
    "    resized_dress = cv2.resize(dress_img, None, fx=scale_factor * 1.5, fy=scale_factor * 1.5)\n",
    "\n",
    "    # Calculate the coordinates for the dress overlay\n",
    "    dress_y = y + int(h * 0.8)  # Adjust the value to position the dress below the neck\n",
    "    dress_x = x - int((resized_dress.shape[1] - w) / 2) - 200\n",
    "\n",
    "    # Ensure dress overlay does not go out of bounds\n",
    "    dress_y_end = min(dress_y + resized_dress.shape[0], frame.shape[0])\n",
    "    dress_x_end = min(dress_x + resized_dress.shape[1], frame.shape[1])\n",
    "\n",
    "    # Calculate the region of interest for the dress overlay\n",
    "    dress_roi_y_start = max(dress_y, 0)\n",
    "    dress_roi_y_end = min(dress_y_end, frame.shape[0])\n",
    "    dress_roi_x_start = max(dress_x, 0)\n",
    "    dress_roi_x_end = min(dress_x_end, frame.shape[1])\n",
    "\n",
    "    # Calculate the region of interest in the dress image\n",
    "    dress_img_roi_y_start = dress_roi_y_start - dress_y\n",
    "    dress_img_roi_y_end = dress_img_roi_y_start + (dress_roi_y_end - dress_roi_y_start)\n",
    "    dress_img_roi_x_start = dress_roi_x_start - dress_x\n",
    "    dress_img_roi_x_end = dress_img_roi_x_start + (dress_roi_x_end - dress_roi_x_start)\n",
    "\n",
    "    # Extract the dress overlay and mask within the region of interest\n",
    "    dress_overlay = resized_dress[dress_img_roi_y_start:dress_img_roi_y_end, dress_img_roi_x_start:dress_img_roi_x_end]\n",
    "    dress_mask = dress_overlay[:, :, -1] / 255.0\n",
    "\n",
    "    # Invert the dress mask\n",
    "    inv_dress_mask = 1.0 - dress_mask\n",
    "\n",
    "    # Apply the dress overlay to the frame within the region of interest\n",
    "    frame[dress_roi_y_start:dress_roi_y_end, dress_roi_x_start:dress_roi_x_end, :3] = (\n",
    "            dress_mask[:, :, np.newaxis] * dress_overlay[:, :, :3] +\n",
    "            inv_dress_mask[:, :, np.newaxis] * frame[dress_roi_y_start:dress_roi_y_end, dress_roi_x_start:dress_roi_x_end, :3]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def process_and_save_output(frame, dress_index, detections, x, y, w, h):\n",
    "    global user_picture_path\n",
    "    if dress_images and user_picture_path:\n",
    "        output_folder = 'outputs'\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        output_path = os.path.join(output_folder, f'output{dress_index}.png')  \n",
    "        frame_copy = frame.copy()\n",
    "        dress_img = dress_images[dress_index]\n",
    "        apply_dress(frame_copy, dress_img, x, y, w, h)\n",
    "        cv2.imwrite(output_path, frame_copy)\n",
    "        print(f'Saved output image: {output_path}')\n",
    "\n",
    "def main():\n",
    "    # Select dress images\n",
    "    select_dress_images()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    current_dress_index = 0\n",
    "    processed_all_dresses = False\n",
    "\n",
    "    while cam.isOpened() and not processed_all_dresses:\n",
    "        ret, frame = cam.read()\n",
    "        if not ret:\n",
    "            print('Error: Failed to open webcam or read frame')\n",
    "            break\n",
    "            \n",
    "        # Convert frame to blob for face detection\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        \n",
    "        # Iterate through detections\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                # Get face coordinates\n",
    "                box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])\n",
    "                (x, y, w, h) = box.astype(int)\n",
    "                \n",
    "                # Apply dress on detected face\n",
    "                if dress_images:\n",
    "                    if time.time() - start_time >= 1:\n",
    "                        dress_index = current_dress_index % len(dress_images)\n",
    "                        process_and_save_output(frame, dress_index, detections, x, y, w, h)\n",
    "                        current_dress_index += 1\n",
    "\n",
    "                        if current_dress_index >= len(dress_images):\n",
    "                            processed_all_dresses = True\n",
    "                            break\n",
    "\n",
    "        # Display frame with dress overlay\n",
    "        cv2.imshow('Virtual Try-On', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('.'):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cam.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
